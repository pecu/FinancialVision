{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DPSGD.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Install `tensorflow_privacy` module from Tensorflow"],"metadata":{"id":"hSCBRifUAPWu"}},{"cell_type":"code","metadata":{"id":"pLYdiCEx3ZSu"},"source":["!pip install tensorflow_privacy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Mount Google Drive for Loading Dataset\n","- The default working directory is at the root of google drive.\n","- You can change it by modifying the path in `os.chdir()`.\n","- `ETH_gaf.pkl` dataset should be placed under the working directory path."],"metadata":{"id":"u6-lZAyhAaUj"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Q-6zq5La5Z56"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import Modules"],"metadata":{"id":"JduozR2BAkzg"}},{"cell_type":"code","metadata":{"id":"_jvRizUI3fTV"},"source":["from absl import app\n","from absl import flags\n","from absl import logging\n","\n","import numpy as np\n","import tensorflow as tf\n","import pickle\n","from tensorflow_privacy.privacy.analysis.rdp_accountant import compute_rdp\n","from tensorflow_privacy.privacy.analysis.rdp_accountant import get_privacy_spent\n","from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasSGDOptimizer\n","\n","import timeit\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E7XlSmDuglKd"},"source":["class Baseline():\n","  def __init__(self,learning_rate,epochs):\n","    self.learning_rate=learning_rate\n","    self.epochs = epochs\n","  \n","  def load_candlestick(self):\n","    fn = \"/content/drive/MyDrive/Colab Notebooks/DP/DPSGD/ETH_gaf.pkl\"\n","    with open(fn, 'rb') as f:\n","        data = pickle.load(f)\n","    return (data['train_culr_gaf'], data['train_onehot'], data['val_culr_gaf'], data['val_onehot'], data['test_culr_gaf'], data['test_onehot']) \n","  \n","  def baseline_model(self):\n","    model = tf.keras.Sequential([\n","          tf.keras.layers.Conv2D(16, 2,\n","                                  strides=(1, 1),\n","                                  padding='same',\n","                                  activation='sigmoid',\n","                                  input_shape=(10, 10, 4)),\n","          \n","          tf.keras.layers.Conv2D(16, 2,\n","                                  strides=(2, 2),\n","                                  padding='same',\n","                                  activation='sigmoid'),\n","          tf.keras.layers.Flatten(),\n","          tf.keras.layers.Dense(256, activation='relu'),\n","          tf.keras.layers.Dense(8,activation=\"softmax\")\n","        ])\n","    return model\n","\n","  def baseline_train(self):\n","    train_data, train_labels, val_data, val_labels, test_data, test_labels = self.load_candlestick()\n","\n","    optimizer = tf.keras.optimizers.SGD(learning_rate=self.learning_rate,momentum= 0.85,nesterov=True)\n","  \n","    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction=tf.losses.Reduction.NONE)\n","    model = self.baseline_model()\n","    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n","    \n","    self.history = model.fit(train_data.astype(np.float32), train_labels.astype(np.float32),\n","          epochs=self.epochs,\n","          validation_data=(val_data.astype(np.float32), val_labels.astype(np.float32)),\n","          batch_size=100)\n","    \n","    test_pred = model.predict(test_data)\n","    test_pred = np.argmax(test_pred, axis=1)\n","    test_true = np.argmax(test_labels, axis=1)\n","\n","    test_result_cm = confusion_matrix(test_true, test_pred, labels=range(8))\n","    print(test_result_cm)\n","    count = 0\n","    for r in range(8):\n","        count += test_result_cm[r, r] \n","    print('testing accuracy:', count/np.sum(test_result_cm))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rQ2r1G6FglXV"},"source":["b_learning_rate = 0.0006\n","b_epochs = 120\n","\n","baseline=Baseline(b_learning_rate, b_epochs)\n","baseline.baseline_train()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Main Training Loop of the DP-SGD Framework"],"metadata":{"id":"ZX67thssFW31"}},{"cell_type":"code","metadata":{"id":"LCnOllGs3Rid"},"source":["class Training():\n","  def __init__(self,l2_norm_clip, noise_multiplier, learning_rate, epochs):\n","    self.l2_norm_clip = l2_norm_clip\n","    self.noise_multiplier = noise_multiplier\n","    self.learning_rate = learning_rate\n","    self.epochs = epochs\n","    \n","  def load_candlestick(self):\n","    fn = \"./ETH_gaf.pkl\"\n","    with open(fn, 'rb') as f:\n","        data = pickle.load(f)\n","    return (data['train_culr_gaf'], data['train_onehot'], data['val_culr_gaf'], data['val_onehot'], data['test_culr_gaf'], data['test_onehot']) \n","    \n","  def create_model(self):\n","    model = tf.keras.Sequential([\n","          tf.keras.layers.Conv2D(16, 2,\n","                                  strides=(1, 1),\n","                                  padding='same',\n","                                  activation='relu',\n","                                  input_shape=(10, 10, 4)),\n","          \n","          tf.keras.layers.Conv2D(16, 2,\n","                                  strides=(2, 2),\n","                                  padding='same',\n","                                  activation='relu'),\n","          tf.keras.layers.Flatten(),\n","          tf.keras.layers.Dense(256, activation='relu'),\n","          tf.keras.layers.Dense(8,activation=\"softmax\")\n","        ])\n","    return model\n","\n","  def compute_epsilon(self,steps):\n","    \"\"\"Computes epsilon value for given hyperparameters.\"\"\"\n","    orders = [1 + x / 10. for x in range(1, 100)] + list(range(12, 64))\n","    sampling_probability = 100 / 13170\n","    rdp = compute_rdp(q=sampling_probability,\n","                      noise_multiplier=self.noise_multiplier,\n","                      steps=steps,\n","                      orders=orders)\n","    return get_privacy_spent(orders, rdp, target_delta=1e-5)[0]\n","\n","  def star_train(self):\n","    train_data, train_labels, val_data, val_labels, test_data, test_labels = self.load_candlestick()\n","\n","    optimizer = DPKerasSGDOptimizer(\n","      l2_norm_clip=self.l2_norm_clip,\n","      noise_multiplier=self.noise_multiplier,\n","      num_microbatches=30,\n","      learning_rate=self.learning_rate, \n","      momentum=0.9\n","      )\n","  \n","    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction=tf.losses.Reduction.NONE)\n","    model = self.create_model()\n","    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n","    self.history = model.fit(train_data.astype(np.float32), train_labels.astype(np.float32),\n","          epochs=self.epochs,\n","          validation_data=(val_data.astype(np.float32), val_labels.astype(np.float32)),\n","          batch_size=30)\n","    \n","    test_pred = model.predict(test_data)\n","    test_pred = np.argmax(test_pred, axis=1)\n","    test_true = np.argmax(test_labels, axis=1)\n","    test_result_cm = confusion_matrix(test_true, test_pred, labels=range(8))\n","    print(test_result_cm)\n","    count = 0\n","    for r in range(8):\n","        count += test_result_cm[r, r] \n","    print('testing accuracy:', count/np.sum(test_result_cm))\n","  \n","    self.eps = self.compute_epsilon(self.epochs * train_data.shape[0] // 100)\n","    print('For delta=1e-5, the current epsilon is: %.2f' % self.eps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IdBbCdLd3vcd"},"source":["l2_norm_clip = [1, 1.5]\n","learning_rate = 0.01\n","noise_multiplier = [0.1, 0.3, 0.5, 0.7, 1]\n","epochs = 120\n","\n","for l2 in l2_norm_clip:\n","  for noise in noise_multiplier:\n","    print(\"=\" * 23, \"l2_norm_clip=\", l2, \" \", \"noise=\", noise, \"=\" * 23)\n","    a = Training(l2, noise, learning_rate, epochs)\n","    a.star_train()"],"execution_count":null,"outputs":[]}]}